---
title: "Time Series - Project"
author: "Mathieu Marauri"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: no
    number_sections: no
    toc: yes
header-includes: \usepackage{float}
---

```{r initial_chunk, echo = FALSE, warning = FALSE, message = FALSE}
library("knitr")
library("ggplot2")
library("xtable")
library("dplyr")
library("gridExtra")
library("zoo")
source('PlotTimeSeriesFunctions.R')
source("outlierTreatment.r")
opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE)
options("xtable.comment" = FALSE)
options("xtable.table.placement" = "H")
```


```{r data}
ipi <- window(ts(read.table("Data/IPI.dat"), start = 1990, freq = 12), start = 1995)
```


# Introduction
For this project we decided to work on the IPI serie. It is a monthly index of industrial production in Spain. The aim of this study is to fit a model to the serie in order to make previsions. During this analysis the outliers will be studied, if some exist, in order to adjust the model fitted.

Figure \ref{plotserie} shows the IPI serie.

```{r serie,fig.cap="IPI serie. \\label{plotserie}", fig.pos = "H"}
tsggplot(ipi)
```


# Identification

## Question a

The first thing that needs to be done is to check whether the variance is constant or not. Several graphs help answer that. 

Figure \ref{varmeanipi} and \ref{boxipi} show the evolution of the variance depending on the mean.
```{r constantvariance,fig.pos="H",fig.align='left',fig.cap="Evolution of the variance relatively to the mean for the original serie. \\label{varmeanipi}"}
m <- apply(matrix(ipi,nr=12),2,mean)
v <- apply(matrix(ipi,nr=12),2,var)
qplot(m,v,xlab="Yearly mean",ylab="Yearly variance") + stat_smooth(method="lm", se=FALSE)
```

One can observe here that the linear relation between the variance and the mean does not have a null slope. It means that the variance is higher whenever the mean is higher.
```{r boxplotipi,fig.pos="H",fig.align='left',fig.cap="Boxplot of the ipi for each year. \\label{boxipi}"}
boxplot(ipi~floor(time(ipi)))
```

One could observe a low variance in 2013 meanwhile in 2006 one could observe a high one. The variance of the original serie does not seem to be constant.

To correct this, a log-transformation is applied to the serie.

The previous graphs are done again in Figure \ref{varmeanlogipi} and \ref{boxlogipi}. The transformation enhances the plots in a way that now one can consider that the variance is constant.
```{r constantvariancelog,fig.pos="H",fig.align='left',fig.cap="Evolution of the variance relatively to the mean for the log of the serie. \\label{varmeanlogipi}"}
logipi <- log(ipi)
m <- apply(matrix(logipi,nr=12),2,mean)
v <- apply(matrix(logipi,nr=12),2,var)
qplot(m,v,xlab="Yearly mean",ylab="Yearly variance ") + 
  stat_smooth(method="lm", se=FALSE)
```

```{r boxplotlogipi,fig.pos="H",fig.align='left',fig.cap="Boxplot of log(ipi) for each year. \\label{boxlogipi}"}
boxplot(logipi~floor(time(logipi)))
```

Now that the serie has been transformed so that it has a constant mean, one has to check if there is seasonal pattern. The following plot (Figure \ref{monthplot}) helps find a potential one.
```{r monthplot,fig.pos="H",fig.align='left',fig.cap="Monthplot of log(ipi). \\label{monthplot}"}
plot(decompose(logipi))
```

It appears that the serie has a annual pattern. Hence a differentiation is needed. 

Then the serie is to have a constant mean. By plotting the transformed serie one can have an idea whether the mean is constant or not. If not the serie needs to be differentiate again. In Figure \ref{plotd12logipi} the logarithm of the serie, after differentiation is plotted.


```{r d12logipi,fig.pos="H",fig.align='left',fig.cap="log(ipi) differentiate 12 times (seasonnality). \\label{plotd12logipi}"}
d12logipi <- diff(logipi,12)
tsggplot(d12logipi,"d12logipi") + geom_hline(y=0)
```

One could say that the mean is not constant so the serie is differentiate another time. The serie is plotted again (Figure \ref{plotd1d12logipi}).
```{r d1d12logipi,fig.pos="H",fig.align='left',fig.cap="log(ipi) differentiate 12 times the 1 time. \\label{plotd1d12logipi}"}
d1d12logipi <- diff(d12logipi,1)
tsggplot(d1d12logipi) + geom_hline(y=0)
```

Now the mean seems to be constant. 

In order to select which transformation is actually the best the variance of the transformed series are compared. The smallest variance is preferred. Table 1 shows the results.

\begin{table}[ht]
\centering
\begin{tabular}{cc}
  \hline
serie & variance \\ 
  \hline
IPI & 255.5872 \\ 
  log(IPI) & 0.0240 \\ 
  $(1-B^12)log(IPI)$ & 0.0050 \\ 
  $(1-B)(1-B^12)log(IPI)$ & 0.0045 \\ 
   \hline
\end{tabular}
\caption{Variances of each transformed serie.} 
\end{table}

The last serie is selected. It is the serie with a log transformation and two differentiation. One for the seasonality and one to have a constant mean. The serie can be written $(1-B)(1-B^12)log(IPI)$.

For now on the transformed serie will be referred simply as teh serie.

## Question b

In order to identify some possible models, the Auto-Correlation Function and the Partial ACF of the serie are plotted. Figure \ref{acfipi}.

```{r acfipi,fig.pos="H",fig.align='left',fig.cap="ACF and PACF of the serie. \\label{acfipi}"}
acfts(ipi)
```

The red lags are used to identify the seasonal part. One can consider an ARMA(3,2) or an ARMA(3,5) looking for the last significant lag. The ACF gives the MA part nd the PACF the AR part. For the regular part, an AR(2) or an AR(6) can be choosen.

Finally 4 models are possible:

* Model 1: $ARIMA(2,0,0)(3,1,2)_12$
* Model 2: $ARIMA(2,0,0)(3,1,5)_12$ 
* Model 3: $ARIMA(6,0,0)(3,1,2)_12$ 
* Model 4: $ARIMA(6,0,0)(3,1,5)_12$ 

# Estimation

```{r models}
ipi.t <- d1d12logipi
mod1bis <- arima(logipi,order=c(2,1,0),seasonal=list(order=c(3,1,2),period=12))
mod2bis <- arima(logipi,order=c(2,1,0),seasonal=list(order=c(3,1,5),period=12))
mod3bis <- arima(logipi,order=c(6,1,0),seasonal=list(order=c(3,1,2),period=12)) 
mod4bis <- arima(logipi,order=c(6,1,0),seasonal=list(order=c(3,1,5),period=12))  
```

Before estimating the models one can try to see if the intercept is needed or not in the models. To do so the ratio $\frac {estimate}{standard error}$ is computed. If it is lower than 2 then the coefficient can be set to zero. In every models the intercept was not significant so it was set to zero. The resulting models have a lower AIC. This confirms that the models are better without the intercept. 

Now the ACF and PACF of each model are plotted and a comparison is made with the ACF and PACF of the serie. This makes possible eliminate 2 models. A residuals analysis will be performed later. 

As an example the comparison between model 3 and the serie is presented in Figure \ref{mod3}. The other graphs are in the section \nameref{appendix}.

Model 3: $ARIMA(6,0,0)(3,1,2)_12$. 
```{r mod3,fig.pos="H",fig.align='left',fig.cap="ACF and PACF of the serie and the model 3. \\label{mod3}"}
acfts(ipi.t)
acfmodel(mod3bis)
```

By comparing the red lags, the one for the seasonality, it appears that the model and the serie have the same signifiant lag with the same sign. The same is true for the first 12 lags that correspond to the regular part. One can conclude that model 3 is a good representation of the serie. 

Such similitudes were also observed for the model 4. Not so much for the models 1 and 2.

Therefore the complete analysis of residuals will be performed only on model 3 and 4.

After performing this analysis the models 3 and 4 were adjusted based on the significance of each coefficient. The unsignificant coefficients were removed and the models were compared using the AIC. The model 4 did not change but the model 3 did. The coefficients for the ar3, ar4 and sma1 were fixed to 0 and the new model was an $ARIMA(6,1,0)(1,1,2)_{12}$ for the logarithm of the serie.

# Validation

## Question a

The analysis of the residuals is a way to validate the model. The residuals must verify the following hypotheses:

* They have the same variance. Homoscedasticity.
* They must follow a normal distribution.
* They have to be independant.

These hypotheses will be assessed graphically. 

### Model 3: $ARIMA(6,1,0)(1,1,2)_12$

We start by checking the homoscedasticity. To do so the residuals are plotted along with a scatter plot with a smooth. Figure \ref{homo3}.
```{r homo3,fig.pos="H",fig.align='left',fig.cap="Residuals and scatter plot of the residuals. \\label{homo3}"}
mod3bis <- arima(logipi,order=c(6,1,0),seasonal=list(order=c(1,1,2),period=12),
                          fixed=c(NA,NA,0,0,NA,NA,NA,0,NA))
resid <- residplot(mod3bis)
scatter <- scatterggplot(mod3bis)
grid.arrange(resid,scatter,ncol=2)
```

Residuals do not go far outside the confidence bounds except for some values (outliers?). The scatter plot does not show any tendancy even if the smooth is not completely straight. The homoscedasticity is verified.

Now the normality of the residuals is to be checked. To do so a qqplot and the histogram of the residuals along with the density of a normal are plotted. Figure \ref{norm3} 
```{r norm3,fig.pos="H",fig.align='left',fig.cap="QQ-plot and histogram of the residuals. \\label{norm3}"}
qqggplot(mod3bis)
hist(mod3bis$residuals,breaks=20,freq=F)
curve(dnorm(x,mean=0,sd=sd(mod3bis$residuals)),col=2,add=T)
```

The normality can be considered verified even if we have quite heavy tails in the QQ-plot and some parts of the histogram are upper than the curve.

Here we check the independence of residuals. Figure \ref{indep3}.
```{r indep3,fig.pos="H",fig.align='left',fig.cap="ACF and PACF of the residuals. \\label{indep3}"}
acfts(mod3bis$residuals)
```

The residuals are independent. Indeed significant lags are far away from the origin.

The next step is to check for volatility. Figure \ref{volatility3}.
```{r volatility3,fig.pos="H",fig.align='left',fig.cap="ACF and PACF of the residualsÂ². \\label{volatility3}"}
acfts(mod3bis$residuals)
```

There is no volatility because the lags from the ACF and PACF of the squared residuals are not outside the confidence bounds, or far away from origin.

The last step is to perform a white noise test. To do so a Ljung-Box is performed. The resulting p-values are presented in Figure \ref{ljung3}.
```{r ljung3,fig.pos="H",fig.align='left',fig.cap="P-values of the Ljung-Box tests. \\label{ljung3}"}
ljungggplot(mod3bis)
```

The fact that some p-values are below 0.05 and so that the null hypothesis of residuals being a white noise is rejected may be due to the fact that the residuals were maybe not really normally distributed.

### Model 4 $ARIMA(6,0,0)(3,1,5)_12$

The same analysis was performed on the model 4. Figures are presented in section \nameref{mod4appendix}. Mainly the same results are obtained except for the white noise tests. As one can see in Figure \ref{ljung4} no p-values are below 0.05. 
```{r ljung4,fig.pos="H",fig.align='left',fig.cap="P-values of the Ljung-Box tests. \\label{ljung4}"}
ljungggplot(mod4bis)
```

As a consequence model 3 seems to be a bit better than model 3. Besides it has a lower AIC (-857 vs -819).

## Question b

A model is stationary and invertible if the AR characteristic polynomial roots and the MA characteristic polynomial roots respectively are greater than 1.

These roots were computed for the models 3 and 4 and they all were greater than 1 so the two models are invertible and stationary. 

## Question c

To check if the model is stable, the coefficients of the serie and those from the serie without the 12 last observation are compared. If they are mostly the same then the model is stable.

Table 2 presents the results for the model 3.
```{r stability3,results='asis'}
ultim=c(2013,12)
ipi <- window(ts(read.table("Data/IPI.dat"), start = 1990, freq = 12), start = 1995)
ipi2 <- window(ipi,end=ultim)
logipi2 <- log(ipi2)
d12logipi2 <- diff(logipi2,12)
d1d12logipi2 <- diff(d12logipi2,1)
ipi2 <- d1d12logipi2
mod3bis2 <- arima(logipi2,order=pdq,seasonal=list(order=PDQ,period=12),fixed=c(NA,NA,0,0,NA,NA,NA,0,NA))
coef3 <- data.frame(mod3bis$coef,mod3bis2$coef)
colnames(coef3) <- c("Complete model","Truncate model")
print(xtable(coef3,align=c("c","c","c"),digits=4,
             caption="Comparison of the coefficients for model 3"))
```

Since the coefficients are the same (mainly) the model 3 can be considered stable. 

Table 4 presents the results for the model 4.
```{r stability4,results='asis'}
mod4bis2 <- arima(logipi2,order=c(6,1,0),seasonal=list(order=c(3,1,5),period=12))
coef4 <- data.frame(mod4bis$coef,mod4bis2$coef)
colnames(coef4) <- c("Complete model","Truncate model")
print(xtable(coef4,align=c("c","c","c"),digits=4,
             caption="Comparison of the coefficients for model 4"))
```

As before the coefficients are the same so the model 4 is stable.

Based on the truncate models forecasts were performed for the last year. It allows to check if the predictions are good or not since the values for the last year are available. Figure \ref{pred3} and \ref{pred4} shows the predictions made respectively with model 3 and 4 along with the confidence interval.
```{r pred3,fig.pos="H",fig.align='left',fig.cap="Predictions and confidence interval for the truncate model 3. \\label{pred3}"}
pred3 <- predict(mod3bis2,n.ahead=12)
pr3 <- ts(c(tail(logipi2,1),pred3$pred),start=ultim,freq=12)
se3 <- ts(c(0,pred$se),start=ultim,freq=12)
tl3 <- ts(exp(pr3-1.96*se3),start=ultim,freq=12)
tu3 <- ts(exp(pr3+1.96*se3),start=ultim,freq=12)
pr3 <- ts(exp(pr3),start=ultim,freq=12)
tspredggplot(ipi,pred=pr3,upperb=tu3,lowerb=tl3)
```

```{r pred4,fig.pos="H",fig.align='left',fig.cap="Predictions and confidence interval for the truncate model 4. \\label{pred4}"}
pred4 <- predict(mod4bis2,n.ahead=12)
pr4 <- ts(c(tail(logipi2,1),pred4$pred),start=ultim,freq=12)
se4 <- ts(c(0,pred4$se),start=ultim,freq=12)
tl4 <- ts(exp(pr4-1.96*se4),start=ultim,freq=12)
tu4 <- ts(exp(pr4+1.96*se4),start=ultim,freq=12)
pr4 <- ts(exp(pr4),start=ultim,freq=12)
tspredggplot(ipi,pred=pr4,upperb=tu4,lowerb=tl4,title="Predictions for model 4.")
```

## Question d
\label{Questiond}

In order to compare the model for the prediction the Mean Square Prediction Error and the Mean Square Absolute Prediction Error are computed for the two models. The lower the values are, the better the model is. 

* $MSPE = \sqrt {\frac {\sum (\frac {(obs-pred)}{obs})^2}{12}}$.
* $MSAPE = \sqrt {\frac {\sum \frac {|obs-pred|}{|obs|}}{12}}$

The model 3 has lower values than model 4. Therefore even if model 4 is better to describe the serie, model 3 is better for the forecast.

# Predictions

Since model 3 is better for predictions (see section \nameref{Questiond}) the long term forecasts will be done with this model. 

Figure \ref{perdl3} displays the predicted values for the serie for the year 2015 along with the confidnce interval. 
```{r predl3,fig.pos="H",fig.align='left',fig.cap="Predictions and confidence interval for the  model 3. \\label{predl3}"}
ipi1 <- window(ipi,end=ultim+c(1,0))
logipi1 <- log(ipi1)
pred <- predict(mod3bis,n.ahead=12)
pr <- ts(c(tail(logipi1,1),pred$pred),start=ultim+c(1,0),freq=12)
se <- ts(c(0,pred$se),start=ultim+c(1,0),freq=12)
tl1<-ts(exp(pr-1.96*se),start=ultim+c(1,0),freq=12)
tu1<-ts(exp(pr+1.96*se),start=ultim+c(1,0),freq=12)
pr1<-ts(exp(pr),start=ultim+c(1,0),freq=12)
tspredggplot(ipi1,pred=pr1,upperb=tu1,lowerb=tl1)
```

# Outliers treatment

## Question a

In this section outliers are analized. For that an automatic detection of the outliers is applied. Table 3 presents the results.

\begin{table}[ht]
\centering
\begin{tabular}{cccc}
  \hline
Observations & Kind & Date & Evolution \\ 
  \hline
 28 & AO & Apr 1997 & 109.08 \\ 
  62 & TC & Feb 2000 & 105.33 \\ 
  88 & AO & Apr 2002 & 107.43 \\ 
  124 & AO & Apr 2005 & 106.99 \\ 
  159 & LS & Mar 2008 & 94.52 \\ 
  160 & AO & Apr 2008 & 108.35 \\ 
  166 & LS & Oct 2008 & 95.32 \\ 
 168 & LS & Dec 2008 & 91.41 \\ 
  175 & LS & Jul 2009 & 107.38 \\ 
  181 & AO & Jan 2010 & 94.41 \\ 
  220 & LS & Apr 2013 & 104.96 \\ 
   \hline
\end{tabular}
\caption{Summary of the outliers.} 
\end{table}

Note that _AO_ stands for additive outlier and _LS_ for level shift. Additive outliers means that it only affects one point. Instead of observing the predicted point on the linearized serie we observe another point. In level shift there is a break in the serie that is not corrected. The effect is permanent. 

We can see that most of the additive outliers are in April, it is maybe due to the years with easter in March instead of April.

2 level shift can be observed in 2008. They are probably due to the financial crisis since the shift is negative.

The last level shift is in July 2009 and has a positive effect. It may be explain by the recovery of the spanish production.

## Question b

Before comparing the predictions of the linearized serie versus the serie the wo serie can be compared and a model must be fitted on the linear serie. 

Figure \ref{obsvslin} shows the two series plotted on the same graph.
```{r obsvslin,fig.pos="H",fig.align='left',fig.cap="Observed vs linearized serie. The linearized serie is in red. \\label{obsvslin}"}
logipi.lin=lineal(logipi,mod.atip3$atip)
ipi.lin=exp(logipi.lin)
ts.data.frame <- data.frame(date=as.Date(as.yearmon(time(ipi))),as.matrix(ipi))
colnames(ts.data.frame) <- c("time","value")
tsl.data.frame <- data.frame(date=as.Date(as.yearmon(time(ipi.lin))),as.matrix(ipi.lin))
colnames(tsl.data.frame) <- c("time","valuel")
ggplot(data=ts.data.frame, mapping=aes(x=time, y=value)) + geom_line() + 
  geom_line(data=tsl.data.frame,aes(x=time,y=valuel),colour="red")
  ggtitle(title) + theme(panel.grid.major.y=element_blank(),
                         panel.grid.minor.y=element_blank()) + ylab("Values of the serie")
```

One can see that the linearized serie is different at the end of the period, after 2008, and that it is above the observed serie.

In the following graph the effects of the outliers on the serie of the logarithm are plotted. Figure \ref{effectout}
```{r effectout,fig.pos="H",fig.align='left',fig.cap="Effects of the outliers. \\label{effectout}"}
tsggplot(logipi-logipi.lin)
```

One can clearly see the different Additive Outliers (only a pick), the Transitive Change for the yeaar 2000 and the several Level Shifts after 2008.

In order to fit a model on the linearized serie the ACF and PACF are plotted in Figure \ref{acflin}. They are done on the two times differentiated serie of the logarithm.
```{r ACF PACF lin,fig.pos="H",fig.align='left',fig.cap="ACF and PACF of the linearized serie. \\label{acflin}"}
acfts(d1d12logipi.lin)
```

After validation, we decided to choose an $ARIMA(6,1,0)(1,1,2)_12$
 
As it was done before the truncated serie is used to get predicion. This way a comparison with the predictions of the model 3 fot the all serie is possible. 

Figure \ref{predlin} shows the predictionsa and the confidence interval obtained with the linearized model.
```{r Predictions for model lineal,fig.pos="H",fig.align='left',fig.cap="Predictions and confidence interval obtained with the linearized truncated model. \\label{predlin}"}
mod3bis.lin <- arima(logipi,order=c(6,1,0),seasonal=list(order=c(1,1,2),period=12),fixed=c(NA,NA,0,0,NA,NA,NA,0,NA))
ultim <- c(2013,12)
pdq <- c(6,1,0)
PDQ <- c(1,1,2)
ipi2.lin <- window(ipi.lin,end=ultim)
logipi2.lin <- log(ipi2.lin)
mod3bis2.lin <- arima(logipi2.lin,order=pdq,seasonal=list(order=PDQ,period=12),fixed=c(NA,NA,0,0,NA,NA,NA,0,NA))
pred <- predict(mod3bis2.lin,n.ahead=12)
pr <- ts(c(tail(logipi2.lin,1),pred$pred),start=ultim,freq=12)
se <- ts(c(0,pred$se),start=ultim,freq=12)
tl1<-ts(exp(pr-1.96*se),start=ultim,freq=12)
tu1<-ts(exp(pr+1.96*se),start=ultim,freq=12)
pr1<-ts(exp(pr),start=ultim,freq=12)
tspredggplot(ipi1,pred=pr1,upperb=tu1,lowerb=tl1)
```

One can notice that the observed values are always in the confidene interval (except at the beginning). 

To have a clear comparison, one can compute the Mean Square Prediction Error and the Mean Square Absolute Prediction Error. To compare the predictions of each model the values of these indicators are compared. As it was done previously small values are preferred.

It appears that the model obtained with the whole serie is better for prediction. It can be explained by the fact that there are a lot of level shift at the end and the linearized serie is above the true one.

Figure \ref{predllin} shows the long term predictions made by the model obtained with the linearized serie.
```{r prellin,fig.pos="H",fig.align='left',fig.cap="Long term predictions and confidence interval obtained with the linearized truncated model. \\label{predllin}"}
ipi1.lin <- window(ipi.lin,end=ultim+c(1,0))
logipi1.lin <- log(ipi1.lin)
pred.lin <- predict(mod3bis.lin,n.ahead=12)
pr.lin <- ts(c(tail(logipi1.lin,1),pred.lin$pred),start=ultim+c(1,0),freq=12)
se.lin <- ts(c(0,pred.lin$se),start=ultim+c(1,0),freq=12)
tl1.lin<-ts(exp(pr.lin-1.96*se.lin),start=ultim+c(1,0),freq=12)
tu1.lin<-ts(exp(pr.lin+1.96*se.lin),start=ultim+c(1,0),freq=12)
pr1.lin<-ts(exp(pr.lin),start=ultim+c(1,0),freq=12)
tspredggplot(ipi,pred=pr1.lin,upperb=tu1.lin,lowerb=tl1.lin)
```


# Appendix

## Comparison of ACF and PACF between models and the serie.

### Model 1: $ARIMA(2,0,0)(3,1,2)_12$ 

```{r mod1,fig.pos="H",fig.align='left',fig.cap="ACF and PACF of the serie and the model 1."}
acfts(ipi.t)
acfmodel(mod1bis)
```


### Model 2: $ARIMA(2,0,0)(3,1,5)_12$

```{r mod2,fig.pos="H",fig.align='left',fig.cap="ACF and PACF of the serie and the model 2."}
acfts(ipi.t)
acfmodel(mod2bis)
```


### Model 3: $ARIMA(6,0,0)(3,1,2)_12$

```{r mod3,fig.pos="H",fig.align='left',fig.cap="ACF and PACF of the serie and the model 3."}
acfts(ipi.t)
acfmodel(mod3bis)
```


### Model 4: $ARIMA(6,0,0)(3,1,5)_12$

```{r mod4,fig.pos="H",fig.align='left',fig.cap="ACF and PACF of the serie and the model 4."}
acfts(ipi.t)
acfmodel(mod4bis)
```

## Residuals analysis of model 4.


```{r homo4,fig.pos="H",fig.align='left',fig.cap="Residuals and scatter plot of the residuals. \\label{homo4}"}
resid <- residplot(mod4bis)
scatter <- scatterggplot(mod4bis)
grid.arrange(resid,scatter,ncol=2)
```


```{r norm4,fig.pos="H",fig.align='left',fig.cap="QQ-plot and histogram of the residuals. \\label{norm4}"}
qqggplot(mod4bis)
hist(mod4bis$residuals,breaks=20,freq=F)
curve(dnorm(x,mean=0,sd=sd(mod4bis$residuals)),col=2,add=T)
```


```{r indep4,fig.pos="H",fig.align='left',fig.cap="ACF and PACF of the residuals. \\label{indep4}"}
acfts(mod4bis$residuals)
```


```{r volatility4,fig.pos="H",fig.align='left',fig.cap="ACF and PACF of the residualsÂ². \\label{volatility4}"}
acfts(mod4bis$residuals)
```


```{r ljung4,fig.pos="H",fig.align='left',fig.cap="P-values of the Ljung-Box tests. \\label{ljung4}"}
ljungggplot(mod4bis)
```
