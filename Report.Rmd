---
title: "Time Series - Project"
author: "Mathieu Marauri"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: no
    number_sections: no
    toc: yes
header-includes: \usepackage{float}
---

```{r initial_chunk, echo = FALSE, warning = FALSE, message = FALSE}
library("knitr")
library("ggplot2")
library("xtable")
library("dplyr")
library("gridExtra")
library("zoo")
source('PlotTimeSeriesFunctions.R')
opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE)
options("xtable.comment" = FALSE)
options("xtable.table.placement" = "H")
```


```{r data}
ipi <- window(ts(read.table("Data/IPI.dat"), start = 1990, freq = 12), start = 1995)
```


# Introduction
For this project we decided to work on the IPI serie. It is a monthly index of industrial production in Spain. The aim of this study is to fit a model to the serie in order to make previsions. During this analysis the outliers will be studied, if some exist, in order to adjust the model fitted.

Figure \ref{plotserie} shows the IPI serie.

```{r serie,fig.cap="IPI serie. \\label{plotserie}", fig.pos = "H"}
tsggplot(ipi)
```


# Identification

## Question a

The first thing that needs to be done is to check whether the variance is constant or not. Several graphs help answer that. 

Figure \ref{varmeanipi} and \ref{boxipi} show the evolution of the variance depending on the mean.
```{r constantvariance,fig.pos="H",fig.align='left',fig.cap="Evolution of the variance relatively to the mean for the original serie. \\label{varmeanipi}"}
m <- apply(matrix(ipi,nr=12),2,mean)
v <- apply(matrix(ipi,nr=12),2,var)
qplot(m,v,xlab="Yearly mean",ylab="Yearly variance") + stat_smooth(method="lm", se=FALSE)
```

One can observe here that the linear relation between the variance and the mean does not have a null slope. It means that the variance is higher whenever the mean is higher.
```{r boxplotipi,fig.pos="H",fig.align='left',fig.cap="Boxplot of the ipi for each year. \\label{boxipi}"}
boxplot(ipi~floor(time(ipi)))
```

One could observe a low variance in 2013 meanwhile in 2006 one could observe a high one. The variance of the original serie does not seem to be constant.

To correct this, a log-transformation is applied to the serie.

The previous graphs are done again in Figure \ref{varmeanlogipi} and \ref{boxlogipi}. The transformation enhances the plots in a way that now one can consider that the variance is constant.
```{r constantvariancelog,fig.pos="H",fig.align='left',fig.cap="Evolution of the variance relatively to the mean for the log of the serie. \\label{varmeanlogipi}"}
logipi <- log(ipi)
m <- apply(matrix(logipi,nr=12),2,mean)
v <- apply(matrix(logipi,nr=12),2,var)
qplot(m,v,xlab="Yearly mean",ylab="Yearly variance ") + 
  stat_smooth(method="lm", se=FALSE)
```

```{r boxplotlogipi,fig.pos="H",fig.align='left',fig.cap="Boxplot of log(ipi) for each year. \\label{boxlogipi}"}
boxplot(logipi~floor(time(logipi)))
```

Now that the serie has been transformed so that it has a constant mean, one has to check if there is seasonal pattern. The following plot (Figure \ref{monthplot}) helps find a potential one.
```{r monthplot,fig.pos="H",fig.align='left',fig.cap="Monthplot of log(ipi). \\label{monthplot}"}
plot(decompose(logipi))
```

It appears that the serie has a annual pattern. Hence a differentiation is needed. 

Then the serie is to have a constant mean. By plotting the transformed serie one can have an idea whether the mean is constant or not. If not the serie needs to be differentiate again. In Figure \ref{plotd12logipi} the logarithm of the serie, after differentatiation is plotted.


```{r d12logipi,fig.pos="H",fig.align='left',fig.cap="log(ipi) differentiate 12 times (seasonnality). \\label{plotd12logipi}"}
d12logipi <- diff(logipi,12)
tsggplot(d12logipi,"d12logipi") + geom_hline(y=0)
```

One could say that the mean is not constant so the serie is differentiate another time. The serie is plotted again (Figure \ref{plotd1d12logipi}).
```{r d1d12logipi,fig.pos="H",fig.align='left',fig.cap="log(ipi) differentiate 12 times the 1 time. \\label{plotd1d12logipi}"}
d1d12logipi <- diff(d12logipi,1)
tsggplot(d1d12logipi) + geom_hline(y=0)
```

Now the mean seems to be constant. 

In order to select which transformation is actually the best the variance of the transformed series are compared. The smallest variance is prefered. Table 1 shows the results.

\begin{table}[ht]
\centering
\begin{tabular}{cc}
  \hline
serie & variance \\ 
  \hline
IPI & 255.5872 \\ 
  log(IPI) & 0.0240 \\ 
  $(1-B^12)log(IPI)$ & 0.0050 \\ 
  $(1-B)(1-B^12)log(IPI)$ & 0.0045 \\ 
   \hline
\end{tabular}
\caption{Variances of each transformed serie.} 
\end{table}

The last serie is selected. It is the serie with a log transformation and two differentiation. One for the seasonality and one to have a constant mean. The serie can be writen $(1-B)(1-B^12)log(IPI)$.

For now on the transformed serie will be referred simply as teh serie.

## Question b

In order to identify some possible models, the Auto-Correlation Function and the Partial ACF of the serie are plotted. Figure \ref{acfipi}.

```{r acfipi,fig.pos="H",fig.align='left',fig.cap="ACF and PACF of the serie. \\label{acfipi}"}
acfts(ipi)
```

The red lags are used to identify the seasonal part. One can consider an ARMA(3,2) or an ARMA(3,5) looking for the last significant lag. The ACF gives the MA part nd the PACF the AR part. For the regular part, an AR(2) or an AR(6) can be choosen.

Finally 4 models are possible:

* Model 1: $ARIMA(2,0,0)(3,1,2)_12$
* Model 2: $ARIMA(2,0,0)(3,1,5)_12$ 
* Model 3: $ARIMA(6,0,0)(3,1,2)_12$ 
* Model 4: $ARIMA(6,0,0)(3,1,5)_12$ 

# Estimation

```{r models}
mod1 <- arima(ipi,order=c(2,0,0),seasonal=list(order=c(3,0,2),period=12))
mod2 <- arima(ipi,order=c(2,0,0),seasonal=list(order=c(3,0,5),period=12))
mod3 <- arima(ipi,order=c(6,0,0),seasonal=list(order=c(3,0,2),period=12)) 
mod4 <- arima(ipi,order=c(6,0,0),seasonal=list(order=c(3,0,5),period=12)) 
```

Before estimating the models one can try to see if the intercept is needed or not in the models. To do so the ratio $\frac {estimate}{standard error}$ is computed. If it is lower than 2 then the coefficient can be set to zero. In every models the intercept was not significant so it was set to zero. The resulting models have a lower AIC. This confirms that the models are better without the intercept. 

Now the ACF and PACF of each model are plotted and a comparison is made with the ACF and PACF of the serie. This makes possibles eliminate 2 models.



We can study those 4 models one by one to select the bests among them before doing a complete analysis of the residual.

## Model 1 $ARIMA(2,0,0)(3,1,2)_12$ 

```{r ACF PACF mod1,echo=FALSE,eval=TRUE}
acfggplot(ipi)
acfmodel(mod1)
# Really good for ACF, quite good for PACF.
```

We compare the ACF and PACF of the model 1 to those from our serie ipi. We can see that the ACF is very close whereas the PACF is not so good.

## Model 2 $ARIMA(2,0,0)(3,1,5)_12$

```{r ACF PACF mode2,echo=FALSE,eval=TRUE}
acfggplot(ipi)
acfmodel(mod2)
# Quite good for ACF and PACF.

```

Here both ACF and PACF are quite good but not as close as we want.

## Model 3 $ARIMA(6,0,0)(3,1,2)_12$

```{r ACF PACF mod3,echo=FALSE,eval=TRUE}
acfggplot(ipi)
acfmodel(mod3)
# Really good for ACF and PACF.
```

For the model 3, we see really good ACF and PACF, close to ipi. It seems that it is one of the best model until now.

## Model 4 $ARIMA(6,0,0)(3,1,5)_12$

```{r ACF PACF mod4,echo=FALSE,eval=TRUE}
acfggplot(ipi)
acfmodel(mod4)
# Really good for ACF and PACF, better than the previous one.
```

Also for the model 4, the ACF and PACF are really good, even better than for model 3.

To conclude this selection, we will perform a complete analysis of residuals on model 3 and 4.

# 3. Validation

## a)

### Model 3 $ARIMA(6,0,0)(3,1,2)_12$

First we can check the homoscedasticity with the following graph
```{r var mod3,echo=FALSE,eval=TRUE}
# Constant variance (homoscedasticity)
resid <- residplot(mod3)
scatter<-
grid.arrange(resid,scatter,ncol=2)
# Pretty good
```

The homoscedasticity is verified here, residuals do not go far outside the confidence bounds and the scatter plot is good too.

We have to check the normality of residuals
```{r norm resid mod3,echo=FALSE,eval=TRUE}
# Normal residuals
qqggplot(mod3)
# Almost ok
hist(mod3$residuals,breaks=20,freq=F)
curve(dnorm(x,mean=0,sd=sd(mod3$residuals)),col=2,add=T)
# Almost ok
```

The normality is almost verified since we have quite heavy tails in this QQ-plot and histogram upper than the curve.

Here we check the independence of residuals
```{r indep resid mod3,echo=FALSE,eval=TRUE}
# Independance of the residuals
acfts(mod3$residuals,"Residuals")
# Independant (significant lag are far away)
```

The residuals are independent. Indeed significant lag are far away from the origin.

The next step is to check for volatility
```{r volatility mod3,echo=FALSE,eval=TRUE}
# Volatility
acfts(mod3$residuals,"ResidualsÂ²")
# No volatility
```

There is no volatility because the lags from the ACF and PACF of the squared residuals are not outside the confidence bounds, or far away from origin.

The last step is to perform a white noise test
```{r wn test mod3,echo=FALSE,eval=TRUE}
# White noise test (above 0.05 => wn)
ljungggplot(mod3)
# Someissues at the end
```
We can see some problems at the end of the plot.

### Model 4 $ARIMA(6,0,0)(3,1,5)_12$

Now we perform the same complete analysis of residuals for the model4

```{r var mod4,echo=FALSE,eval=TRUE}
# Constant variance (homoscedasticity)
resid <- residplot(mod4)
scatter <- scatterggplot(mod4)
grid.arrange(resid,scatter,ncol=2)
# Some outliers but seem constant
```

The variance seems constant even if there are some outliers.

```{r norm resid mod4,echo=FALSE,eval=TRUE}
# Normal residuals
qqggplot(mod4)
# Almost ok

hist(mod4$residuals,breaks=20,freq=F)
curve(dnorm(x,mean=0,sd=sd(mod4$residuals)),col=2,add=T)
# Seem normal

```

The residuals seem normal, the tails are not heavy and the curve quite match the histogram.


```{r indep resid mod4,echo=FALSE,eval=TRUE}
# Independance of the residuals
acfts(mod4$residuals,"Residuals")
# Independant
```

The residuals are independent, there is no significant lag even far from the origin.

```{r volatility mod4,echo=FALSE,eval=TRUE}

# Volatility
acfts(mod4$residuals,"ResidualsÂ²")
# No volatility
```

There is no volatility as there is no significant lag in this plot of ACF and PACF of squared residuals.

```{r wn test mod4,echo=FALSE,eval=TRUE}
# White noise test (above 0.05 => wn)
ljungggplot(mod4)
# OK
```

The test confirm that we have white noises in the model4.

## b)

To see if the model is stationary and invertible, we check is the roots of the polynoms of AR and MA part are greater than 1.

### Model 3 $ARIMA(6,0,0)(3,1,2)_12$

```{r stat and invert mod3,echo=FALSE,eval=TRUE}
cat("\nModul of AR Characteristic polynomial Roots: ", 
    Mod(polyroot(c(1,-mod3$model$phi))),"\n")
cat("\nModul of MA Characteristic polynomial Roots: ",
    Mod(polyroot(c(1,mod3$model$theta))),"\n")
# Stationary and invertible
```

From those values we can conclude that the model 3 is stationary and invertible.

### Model 4 $ARIMA(6,0,0)(3,1,5)_12$

```{r}
cat("\nModul of AR Characteristic polynomial Roots: ", 
    Mod(polyroot(c(1,-mod4$model$phi))),"\n")
cat("\nModul of MA Characteristic polynomial Roots: ",
    Mod(polyroot(c(1,mod4$model$theta))),"\n")
# Stationary and invertible

```

Also here the model 4 is stationary and invertible.


## c)

To check if the model is stable, we compare the coefficient of the serie and those from the serie without the 12 last observation.

```{r stability ,echo=FALSE,eval=TRUE}
ultim=c(2013,12)
ipi <- window(ts(read.table("Data/IPI.dat"), start = 1990, freq = 12), start = 1995)

ipi2 <- window(ipi,end=ultim)
logipi2 <- log(ipi2)
d12logipi2 <- diff(logipi2,12)
d1d12logipi2 <- diff(d12logipi2,1)
ipi2 <- d1d12logipi2
# We look at the same serie without the last 12 observations.

mod3bis <- arima(ipi2,order=c(6,0,0),seasonal=list(order=c(3,0,2),period=12))
mod4bis <- arima(ipi2,order=c(6,0,0),seasonal=list(order=c(3,0,5),period=12))
```

### Model 3 $ARIMA(6,0,0)(3,1,2)_12$

```{r stability mod3,echo=FALSE,eval=TRUE}
mod3$coef
mod3bis$coef
# The modelis stable.
```
Note that _mod3bis_ stands for the serie without the 12 last observations. The coefficient are not different, we can conclude that the model 3 is stable

### Model 4 $ARIMA(6,0,0)(3,1,5)_12$

```{r stability mod4,echo=FALSE,eval=TRUE}
mod4$coef
mod4bis$coef
# The modelis stable.
```
Also here the model 4 is stable.
